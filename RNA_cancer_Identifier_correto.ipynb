{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image  # Importe a classe Image da biblioteca PIL\n",
    "# import helper\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leia o arquivo Excel\n",
    "df = pd.read_excel('C:/Users/Karol/Documents/Python projeto/microscopy_ground_truth.xlsx')\n",
    "\n",
    "# Exiba as primeiras linhas do DataFrame para verificar se os dados foram lidos corretamente\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('Benign', 'InSitu', 'Invasive', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBBx4ICVVC2m",
    "outputId": "c23c5ee9-475d-465e-d246-f8f2430636bc"
   },
   "outputs": [],
   "source": [
    "# Diretório onde estão localizadas as imagens\n",
    "diretorio_imagens = 'C:/Users/Karol/Documents/Python projeto/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/BACH_Preparado/200x200_pb/'\n",
    "\n",
    "# Crie uma lista de caminhos para as imagens\n",
    "image_paths = [diretorio_imagens + nome_da_imagem for nome_da_imagem in df['nome_da_imagem']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instancie o LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Atribua um número único a cada tipo (rótulo)\n",
    "df['tipo_numerico'] = label_encoder.fit_transform(df['tipo'])\n",
    "\n",
    "# Exiba as primeiras linhas do DataFrame com os tipos numéricos\n",
    "print(df[['tipo', 'tipo_numerico']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EUyTSFVVusl"
   },
   "outputs": [],
   "source": [
    "PATH='C:/Users/Karol/Documents/Python projeto/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/BACH_Preparado/200x200_pb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwiJuMeQWU-y"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "                    [transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zn1oXZwbVmnm"
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(PATH, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvDFXs4PWn8S"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KylEBR8DTxH"
   },
   "outputs": [],
   "source": [
    "def imshow(img): \n",
    "    img = (img * 0.5) + 0.5  # desnormalizar imagens\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um batch de dados de treino\n",
    "dataiter = iter(dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "for idx in np.arange(4):\n",
    "    ax = fig.add_subplot(2, 8 // 2, idx + 1, xticks=[], yticks=[])  # Correção na divisão\n",
    "    image = images[idx].numpy().transpose((1, 2, 0))  # Reorganize as dimensões da imagem\n",
    "    image_pil = Image.fromarray((image * 255).astype(np.uint8))  # Converte para objeto PIL\n",
    "    plt.imshow(image_pil)  # Exibe a imagem PIL\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeSKPiVlc2Bb",
    "outputId": "1562f362-f320-4baa-d4c3-6b614c8d5175"
   },
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDlXf6Zoc4pb",
    "outputId": "a3bdbe42-e18a-40e0-c997-0964ba927adb"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXOOLdwvc8A_"
   },
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "fim = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq9wVC5Nc-G5",
    "outputId": "d32ece57-04c6-47a5-9cf2-2e5273d2478f"
   },
   "outputs": [],
   "source": [
    "print (fim - inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpoEaSY7LWzn"
   },
   "source": [
    "Para calcular o tamanho espacial da saida podemos usar a fórmula ((N − F + 2P) / S) + 1.\n",
    "onde:\n",
    "N : Dimensão da imagem de entrada\n",
    "F : Dimensão do Filtro\n",
    "P : O padding\n",
    "S : O stride\n",
    "Lembrando que um maxpooling de (kernel_size = 2,stride = 2) diminui pela metade a dimensão da imagem nos eixos x e y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['tipo_numerico'] = label_encoder.fit_transform(df['tipo'])\n",
    "\n",
    "# Defina sua arquitetura CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # com base nas dimensões da imagem de entrada (200x200)\n",
    "        self.fc_input_size = 32 * 50 * 50  # (32 canais * 50 * 50 pixels)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.fc_input_size)  # Achatamento para as camadas fully connected\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFN0wHnwPENK"
   },
   "outputs": [],
   "source": [
    "#Caso esteja disponível, processar na GPU\n",
    "USAR_GPU = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoDynSyCB7Nn"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# train_loader=torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "# optimizer = optim.Adam(cnn.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHpDu3sCPAAc",
    "outputId": "f0cd21ac-b11e-45bb-9b51-37142ac9f03d"
   },
   "outputs": [],
   "source": [
    "use_cuda = USAR_GPU and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print('Usar GPU: ', USAR_GPU)\n",
    "print('GPU disponivel:', torch.cuda.is_available())\n",
    "print('Processando em:', device )\n",
    "\n",
    "#Seta onde será executada\n",
    "cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCim26XrLx0m",
    "outputId": "29a6b2f6-08fe-4b23-f961-2187ece02547",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y_acertos = []\n",
    "y_loss = []\n",
    "epochs = 40\n",
    "inicio = time.time()\n",
    "\n",
    "# Inicie o treinamento\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_acertos = 0\n",
    "    \n",
    "    # Itere pelo DataLoader\n",
    "    for images, labels in dataloader:\n",
    "        #Coloca os dados no dispositivo de processamento\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Restante do código para treinamento da rede\n",
    "        preds = cnn(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cálculos de acertos e perda\n",
    "        acertos = preds.argmax(dim=1).eq(labels).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_acertos += acertos\n",
    "    \n",
    "    # Armazene as métricas para plotagem ou acompanhamento\n",
    "    x.append(epoch)\n",
    "    y_acertos.append(total_acertos)\n",
    "    y_loss.append(total_loss)\n",
    "\n",
    "    # Exiba o progresso\n",
    "    print('epoch:', epoch, end='')\n",
    "    print('\\tloss: ', round(total_loss, 2), end='')\n",
    "    print('\\tacertos:', total_acertos, f'({round(total_acertos * 100 / len(df), 2)}%)')\n",
    "\n",
    "print(\"Treinamento concluído!\")\n",
    "fim = time.time()\n",
    "tempo = round(fim - inicio, 2)\n",
    "minutos = tempo // 60\n",
    "segundos = round (tempo % 60,2)\n",
    "print(minutos,'minutos e' , segundos, 'segundos' )\n",
    "print(tempo, 'segundos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz9HAeA7cAfS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_path = './model/'\n",
    "file_name = 'cnn_model.pth'\n",
    "\n",
    "if not os.path.exists(model_path): \n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "torch.save(cnn.state_dict(), model_path + file_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "id": "k99PG2jAcAh1",
    "outputId": "8a491d2f-e8d3-40a1-b5e3-0f562ca001c1"
   },
   "outputs": [],
   "source": [
    "# Um batch de dados de treino\n",
    "dataiter = iter(dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "for idx in np.arange(4):\n",
    "    ax = fig.add_subplot(2, 8 // 2, idx + 1, xticks=[], yticks=[])  # Correção na divisão\n",
    "    image = images[idx].numpy().transpose((1, 2, 0))  # Reorganize as dimensões da imagem\n",
    "    image_pil = Image.fromarray((image * 255).astype(np.uint8))  # Converte para objeto PIL\n",
    "    plt.imshow(image_pil)  # Exibe a imagem PIL\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fNDeRqUcAkN",
    "outputId": "25de1589-7dbc-4fc1-d3ce-857d797563b5"
   },
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load(model_path + file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qoulndMcAml",
    "outputId": "fc24d5a5-65d1-45bc-8126-d307bd2197e9"
   },
   "outputs": [],
   "source": [
    "outputs = cnn(images)\n",
    "outputs\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8UvM1hicAsC",
    "outputId": "db8fe785-ce40-4742-abc2-e0db25c5cb20"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# nao precisamos calcular os valores de gradiente pq não estamos treinando\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        outputs = cnn(images)\n",
    "        # a classe com a maior energia é o que escolhemos como previsão\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gidkqldMcGNq",
    "outputId": "a70ac5cd-4d39-4f70-9906-fb4cc6db4dc3"
   },
   "outputs": [],
   "source": [
    "# contar as previsões para cada classe\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# novamente sem gradientes necessários\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        outputs = cnn(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # calcula predições corretas para cada classe\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print acurácia para cada classe\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))\n",
    "\n",
    "# class TestSquare(TestCase): \n",
    "#     def test_classe(self): \n",
    "#         result = accuracy\n",
    "#         expected = 90.0\n",
    "#         self.assertEqual(result, expected)     \n",
    "#         print(classname, accuracy)\n",
    "# if __name__ == '__main__':\n",
    "#     s = unittest.TestLoader().loadTestsFromTestCase(TestSquare)\n",
    "#   unittest.TextTestRunner().run(s)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmAQGm55NRQw"
   },
   "source": [
    "- Porcentagem % -  método para testar,\n",
    "para saber a porcentagem para validar se\n",
    "está acima ou abaixo do limite estipulado\n",
    "\n",
    "- E quantidade de acertos em número\n",
    "\n",
    "- Validar se está passando por\n",
    "todas as imagens\n",
    "\n",
    "- Validar a acurácia de cada tipo\n",
    " ('Benign', 'InSitu', 'Invasive', 'Normal')\n",
    "e ver qual tipo teve maior acerto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59-3Py7oLx0u"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "oASAyy_sLx00",
    "outputId": "925deed6-a107-4e04-b710-983f19a08737"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y_acertos)\n",
    "#plt.ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "OuCPIdbkLx05",
    "outputId": "09b92b04-67fc-41c6-cecb-d1e797b73445"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y_loss)\n",
    "#plt.ylim(bottom=0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-Wnzib6Lx1C"
   },
   "source": [
    "REALIZANDO TESTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# image_path = 'C:/Users/Karol/Documents/Python projeto/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/BACH_Preparado/200x200_pb/Normal/n003.tif'\n",
    "image_path = 'C:/Users/Karol/Documents/Python projeto/ICIAR2018_BACH_Challenge/ICIAR2018_BACH_Challenge/Photos/Invasive/iv008.tif'\n",
    "\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),  # Redimensionar para o tamanho de entrada da rede\n",
    "    transforms.Grayscale(num_output_channels=3),  # Converter para escala de cinza com três canais\n",
    "    transforms.ToTensor(),  # Converter para tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizar\n",
    "])\n",
    "input_image = transform(image).unsqueeze(0)  # Adicionar uma dimensão de lote (batch) única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Desativar o cálculo de gradientes para economizar memória\n",
    "    outputs = cnn(input_image)\n",
    "    _, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe_predita = classes[predicted.item()]\n",
    "print(f'A classe prevista para a imagem é: {classe_predita}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
